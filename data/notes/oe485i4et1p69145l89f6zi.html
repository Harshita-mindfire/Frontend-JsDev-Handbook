<h1 id="langchain">Langchain<a aria-hidden="true" class="anchor-heading icon-link" href="#langchain"></a></h1>
<ul>
<li>open source development framework for LLM applications.</li>
<li>A framework that helps you build LLM-powered apps by handling prompts, memory, tools, and data so you don‚Äôt have to reinvent the wheel.</li>
<li>two packages: python and JS(TS)</li>
</ul>
<p>With LangChain, you get:</p>
<ul>
<li><strong>Prompt templates</strong></li>
<li><strong>Chains</strong> (step-by-step LLM workflows)</li>
<li><strong>Agents</strong> (LLM decides what tool to use)</li>
<li><strong>Memory</strong> (conversation state)</li>
<li><strong>Retrieval</strong> (RAG with vector databases)</li>
<li><strong>Tool calling</strong> (search, DBs, APIs, code)</li>
</ul>
<h2 id="memory">Memory<a aria-hidden="true" class="anchor-heading icon-link" href="#memory"></a></h2>
<p>Memory lets the app remember:</p>
<ul>
<li>Previous messages</li>
<li>User preferences</li>
<li>Intermediate steps</li>
</ul>
<p>Very useful for chatbots, assistants, and copilots.</p>
<h3 id="memory-types">Memory types<a aria-hidden="true" class="anchor-heading icon-link" href="#memory-types"></a></h3>
<p><strong>ConversationBufferMemory</strong></p>
<p>This memory allows for storing of messages and then extracts the messages in a variable.</p>
<p><strong>ConversationBufferWindowMemory</strong></p>
<p>This memory keeps a list of the interactions of the conversation over time. It only uses the last K interactions.</p>
<p><strong>ConversationTokenBufferMemory</strong></p>
<p>This memory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.</p>
<p><strong>ConversationSummaryMemory</strong></p>
<p>This memory creates a summary of the conversation over time.</p>
<h3 id="additional-memory-types">Additional Memory Types<a aria-hidden="true" class="anchor-heading icon-link" href="#additional-memory-types"></a></h3>
<p><strong>Vector data memory</strong> </p>
<p>Stores text (from conversation or elsewhere) in a vector database and retrieves the most relevant blocks of text.</p>
<p><strong>Entity memories</strong></p>
<p>Using an LLM, it remembers details about specific entities.</p>
<p>You can also use multiple memories at one time.</p>
<p>E.g., Conversation memory + Entity memory to recall individuals.</p>
<p>You can also store the conversation in a conventional database (such as key-value store or SQL)</p>
<h2 id="chains">Chains<a aria-hidden="true" class="anchor-heading icon-link" href="#chains"></a></h2>
<h3 id="llm-chain">LLM Chain<a aria-hidden="true" class="anchor-heading icon-link" href="#llm-chain"></a></h3>
<p>An LLM Chain is:</p>
<blockquote>
<p>A repeatable, parameterized, composable workflow around LLM calls.</p>
<ul>
<li>LLMChain is a primitive / building-block chain.</li>
<li>wraps one prompt + one model call</li>
</ul>
</blockquote>
<pre class="language-py"><code class="language-py"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chat_models <span class="token keyword">import</span> ChatOpenAI 
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> ChatPromptTemplate 
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChain

llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> model<span class="token operator">=</span>llm_model<span class="token punctuation">)</span> 

prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span> <span class="token string">"What is the best name to describe \ a company that makes {product}?"</span> <span class="token punctuation">)</span> 

chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>prompt<span class="token punctuation">)</span> 
product <span class="token operator">=</span> <span class="token string">"Queen Size Sheet Set"</span> 
chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>product<span class="token punctuation">)</span> <span class="token comment">#Royal Beddings or anything the gpt responds with</span>
</code></pre>
<h3 id="sequentialchain">SequentialChain<a aria-hidden="true" class="anchor-heading icon-link" href="#sequentialchain"></a></h3>
<ul>
<li>wires multiple chains together where o/p of one chain is i/p of the next chain.
2 types of sequential chains: </li>
<li><strong>SimpleSequentialChain</strong>: single i/p and o/p</li>
<li><strong>SequentialChain</strong>: any step of the chain can take multiple i/p and o/p</li>
</ul>
<p><strong>SimpleSequentialChain</strong></p>
<pre class="language-py"><code class="language-py"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> SimpleSequentialChain
llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> model<span class="token operator">=</span>llm_model<span class="token punctuation">)</span>

<span class="token comment"># prompt template 1</span>
first_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    "What <span class="token keyword">is</span> the best name to describe \
    a company that makes <span class="token punctuation">{</span>product<span class="token punctuation">}</span>?"
<span class="token punctuation">)</span>

<span class="token comment"># Chain 1</span>
chain_one <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>first_prompt<span class="token punctuation">)</span>

<span class="token comment"># prompt template 2</span>
second_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    "Write a <span class="token number">20</span> words description <span class="token keyword">for</span> the following \
    company<span class="token punctuation">:</span><span class="token punctuation">{</span>company_name<span class="token punctuation">}</span>"
<span class="token punctuation">)</span>
<span class="token comment"># chain 2</span>
chain_two <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>second_prompt<span class="token punctuation">)</span>
overall_simple_chain <span class="token operator">=</span> SimpleSequentialChain<span class="token punctuation">(</span>chains<span class="token operator">=</span><span class="token punctuation">[</span>chain_one<span class="token punctuation">,</span> chain_two<span class="token punctuation">]</span><span class="token punctuation">,</span>
overall_simple_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>product<span class="token punctuation">)</span>
</code></pre>
<p><strong>SequentialChain</strong></p>
<pre class="language-py"><code class="language-py"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> SequentialChain
llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> model<span class="token operator">=</span>llm_model<span class="token punctuation">)</span>

<span class="token comment"># prompt template 1: translate to english</span>
first_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    <span class="token string">"Translate the following review to english:"</span>
    <span class="token string">"\n\n{Review}"</span>
<span class="token punctuation">)</span>
<span class="token comment"># chain 1: input= Review and output= English_Review</span>
chain_one <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>first_prompt<span class="token punctuation">,</span> 
                     output_key<span class="token operator">=</span><span class="token string">"English_Review"</span>
                    <span class="token punctuation">)</span>
second_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    <span class="token string">"Can you summarize the following review in 1 sentence:"</span>
    <span class="token string">"\n\n{English_Review}"</span>
<span class="token punctuation">)</span>
<span class="token comment"># chain 2: input= English_Review and output= summary</span>
chain_two <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>second_prompt<span class="token punctuation">,</span> 
                     output_key<span class="token operator">=</span><span class="token string">"summary"</span>
                    <span class="token punctuation">)</span>
<span class="token comment"># prompt template 3: translate to english</span>
third_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    <span class="token string">"What language is the following review:\n\n{Review}"</span>
<span class="token punctuation">)</span>
<span class="token comment"># chain 3: input= Review and output= language</span>
chain_three <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>third_prompt<span class="token punctuation">,</span>
                       output_key<span class="token operator">=</span><span class="token string">"language"</span>
                      <span class="token punctuation">)</span>

<span class="token comment"># prompt template 4: follow up message</span>
fourth_prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>
    <span class="token string">"Write a follow up response to the following "</span>
    <span class="token string">"summary in the specified language:"</span>
    <span class="token string">"\n\nSummary: {summary}\n\nLanguage: {language}"</span>
<span class="token punctuation">)</span>
<span class="token comment"># chain 4: input= summary, language and output= followup_message</span>
chain_four <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>fourth_prompt<span class="token punctuation">,</span>
                      output_key<span class="token operator">=</span><span class="token string">"followup_message"</span>
                     <span class="token punctuation">)</span>
<span class="token comment"># overall_chain: input= Review </span>
<span class="token comment"># and output= English_Review,summary, followup_message</span>
overall_chain <span class="token operator">=</span> SequentialChain<span class="token punctuation">(</span>
    chains<span class="token operator">=</span><span class="token punctuation">[</span>chain_one<span class="token punctuation">,</span> chain_two<span class="token punctuation">,</span> chain_three<span class="token punctuation">,</span> chain_four<span class="token punctuation">]</span><span class="token punctuation">,</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"Review"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    output_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"English_Review"</span><span class="token punctuation">,</span> <span class="token string">"summary"</span><span class="token punctuation">,</span><span class="token string">"followup_message"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>

review <span class="token operator">=</span> <span class="token string">"add some review here"</span>
overall_chain<span class="token punctuation">(</span>review<span class="token punctuation">)</span>
</code></pre>
<h3 id="router-chain">Router Chain<a aria-hidden="true" class="anchor-heading icon-link" href="#router-chain"></a></h3>
<p>A Router Chain is a LangChain pattern used when you don‚Äôt want one single prompt or chain to handle every input, but instead want the system to decide which chain should run based on the input.</p>
<p>Think of it as an LLM-powered if / else or switch statement.
<strong>The core idea</strong> : Given an input, choose the most appropriate chain to handle it.
Example:
If input is a math question ‚Üí use math chain
If input is code-related ‚Üí use coding chain
If input is customer support ‚Üí use support chain
You route the request to the right specialist</p>
<h3 id="key-benefits-of-using-chain">Key benefits of using Chain<a aria-hidden="true" class="anchor-heading icon-link" href="#key-benefits-of-using-chain"></a></h3>
<p>üîπ <strong>1. Prompt templating &#x26; variable injection</strong>
This becomes powerful when: Prompts are long, Variables are many, Prompts are reused across files.</p>
<p><strong>üîπ 2. Reusability &#x26; maintainability</strong>
You define the prompt once, Instead of copy-pasting prompts everywhere.</p>
<pre class="language-py"><code class="language-py">chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"Queen Size Sheet Set"</span><span class="token punctuation">)</span>
chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"Organic Soap"</span><span class="token punctuation">)</span>
chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"AI SaaS Platform"</span><span class="token punctuation">)</span>
</code></pre>
<p><strong>üîπ 3. Easy model swapping &#x26; configuration</strong>
Change model/temperature without touching your logic.</p>
<pre class="language-py"><code class="language-py">llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"gpt-4"</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre>
<p><strong>üîπ 4. Multi-step workflows (SequentialChain)</strong>
While an LLMChain represents a single step, real power comes from composing multiple LLMChains using SequentialChain, RouterChain, or Agents.</p>
<p><strong>üîπ 5. Composition(chains calling chains) with tools, memory, retrievers</strong></p>
<p>Chains can be stacked:</p>
<pre class="language-py"><code class="language-py">User Input
   ‚Üì
Summarization Chain
   ‚Üì
Translation Chain
   ‚Üì
Tone Adjustment Chain
   ‚Üì
Final Output
</code></pre>
<p>Chains can integrate: üîç Vector databases, üß† Conversation memory, üõ†Ô∏è Tools &#x26; function calling, üîÑ Retries &#x26; fallback models, üìä Tracing &#x26; observability</p>
<p>Example:</p>
<pre class="language-py"><code class="language-py">
User Question
   ‚Üì
Retrieve relevant docs
   ‚Üì
Insert into prompt
   ‚Üì
LLM answers <span class="token keyword">with</span> citations
</code></pre>
<p>This is the backbone of RAG (Retrieval-Augmented Generation).</p>
<h2 id="retrieval-rag">Retrieval (RAG)<a aria-hidden="true" class="anchor-heading icon-link" href="#retrieval-rag"></a></h2>
<p>LangChain makes it easy to:</p>
<ul>
<li>Load documents (PDFs, Notion, websites)</li>
<li>Embed them into vectors</li>
<li>Store them in a vector database</li>
<li>Retrieve relevant chunks</li>
<li>Inject them into the prompt</li>
</ul>
<p>This pattern is called <strong>Retrieval-Augmented Generation (RAG)</strong>.</p>
<h3 id="common-components-of-ragretrieval-augmented-generation">Common components of RAG(Retrieval Augmented Generation)<a aria-hidden="true" class="anchor-heading icon-link" href="#common-components-of-ragretrieval-augmented-generation"></a></h3>
<div class="mermaid">
    graph TD;
      Data-Source-->Data-Transformation;
      Data-Transformation-->Text-embedding;
      Text-embedding-->VectorStore-DB;
</div>
<ul>
<li>
<p><strong>Data-Ingestion</strong>: (Load data into langchain/Data ingestion)</p>
</li>
<li>
<p><strong>Data-Transformation</strong> : (breaking data into text chunks)</p>
</li>
<li>
<p><strong>Embedding</strong>: converting text into vectors using embedding techniques. This is required for different algorithms to run(similarity serach). </p>
</li>
<li>
<p><strong>VectorStore-DB</strong>: saving vectors into a vector DB ex: chromaDB, FAISS, ASTRADB</p>
</li>
</ul>
<p>The details are present in their invdiviual pages below</p>
<h3 id="data-ingestion">Data Ingestion:<a aria-hidden="true" class="anchor-heading icon-link" href="#data-ingestion"></a></h3>
<p><a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">learning.langchain.document-ingestion (Private)</a></p>
<h3 id="data-transformationtext-splitting">Data-Transformation(Text splitting)<a aria-hidden="true" class="anchor-heading icon-link" href="#data-transformationtext-splitting"></a></h3>
<p><a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">learning.langchain.text-splitting (Private)</a></p>
<h3 id="embeddings">Embeddings<a aria-hidden="true" class="anchor-heading icon-link" href="#embeddings"></a></h3>
<p><a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">learning.langchain.embeddings (Private)</a></p>
<h2 id="tools">Tools<a aria-hidden="true" class="anchor-heading icon-link" href="#tools"></a></h2>
<p>Tools can be things like:</p>
<ul>
<li>Web search</li>
<li>SQL queries</li>
<li>Python execution</li>
<li>APIs</li>
<li>File systems</li>
</ul>
<p>LangChain gives a standard way to expose these tools to an LLM.</p>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/Frontend-JsDev-Handbook/notes/duydba1xzdaypg4p9egmt3g">Document Ingestion</a></li>
<li><a href="/Frontend-JsDev-Handbook/notes/nrx71p01v1xopk4xtim8xjb">LLM-models</a></li>
<li><a href="/Frontend-JsDev-Handbook/notes/0jmd9qrvoy9orbl1n2mzxla">Simple Gen AI APP Using Langchain</a></li>
<li><a href="/Frontend-JsDev-Handbook/notes/1rujyxrb9vcc5vpxg0s0o8c">Text splitting from Documents</a></li>
<li><a href="/Frontend-JsDev-Handbook/notes/g9anqrp0zrf7ge81edgx8yo">Version-1</a></li>
<li><a href="/Frontend-JsDev-Handbook/notes/qxj02f4em65xllkz33rnbb2">embeddings</a></li>
<li><a href="/Frontend-JsDev-Handbook/notes/g8ci1ind9asxblmph23fcew">vector-store</a></li>
</ol>